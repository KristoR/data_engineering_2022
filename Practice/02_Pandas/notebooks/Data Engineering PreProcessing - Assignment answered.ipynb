{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "<p float=\"left\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/3/39/Tartu_%C3%9Clikool_logo.svg\" width=\"100\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three files that describe some data of mobile phones.\n",
    "- Mobile data 1.csv: This file contains\n",
    "    - id - ID\n",
    "    - battery_power - Total energy a battery can store in one time measured in mAh\n",
    "    - blue - Has bluetooth or not\n",
    "    - clock_speed - speed at which microprocessor executes instructions\n",
    "    - dual_sim - Has dual sim support or not\n",
    "    - fc - Front Camera mega pixels\n",
    "    - four_g - Has 4G or not\n",
    "    - int_memory - Internal Memory in Gigabytes\n",
    "    - m_dep - Mobile Depth in cm\n",
    "    - mobile_wt - Weight of mobile phone\n",
    "    - n_cores - Number of cores of processor\n",
    "    - pc - Primary Camera mega pixels\n",
    "    - px_height - Pixel Resolution Height\n",
    "    - px_width - Pixel Resolution Width\n",
    "    - ram - Random Access Memory in Megabytes\n",
    "    - sc_h - Screen Height of mobile in cm\n",
    "    - sc_w - Screen Width of mobile in cm\n",
    "    - talk_time - longest time that a single battery charge will last when you are\n",
    "    - three_g - Has 3G or not\n",
    "    - touch_screen - Has touch screen or not\n",
    "    - wifi - Has wifi or not\n",
    "    - price_range - This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).\n",
    "- Mobile data 2.csv: This file contains\n",
    "    - ID\t\n",
    "    - Brand\t- the make of the phone, e.g., Apple, Samsunig, Ericsson, etc.\n",
    "    - Phone\t- the model fo the phone, e.g. iPhone 12\n",
    "    - Picture URL small - a hyperlink to an image of the phone\n",
    "    - Body Dimensions - this is compound value that is not uniform. It contains  w x h x d in both mm and inches\n",
    "    - Body Weight - the information is given also in grams and lb but also for several configurations\n",
    "    - Display Resolution - this column combines different values of w x h pixels, screen ratio and pixel density\n",
    "- Price ranges.csv - simple file that contains the range id and the min and max value for each range\n",
    "\n",
    "### Our objective is to merge the data into one data set. \n",
    "### But, first, we have to make some pre-processing on some of the individual files\n",
    "For mobile data 1.csv, we need to make the following pre-processing (transformations)\n",
    "- Remove duplicates\n",
    "- Rename columns\n",
    "- Handle missing data (this will be revisited with more details when talking about data cleansing)\n",
    " - Just drop the rows\n",
    " - Fill with some neighbor value\n",
    " - Fill with some statistical value (mean or median)\n",
    "- Handling outliers\n",
    "- Standardizing the data\n",
    "\n",
    "For mobile data 2.csv, we need to make the following pre-processing (transformations)\n",
    "\n",
    "- Out of the Body dimensions column, we need to extract three columns: Width, Height, Depth all in mm. **This is a one to one transformation.**\n",
    "- Out of the Body weight, we need to extract the weight in grams for each possible offering. For example, for iPads, ones that come with WiFi only have a different weight different from those that come with additional 4G support. **This is a one to many transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the first data set\n",
    "\n",
    "### Profiling\n",
    "First we do some generic profiling. We check the shape, the data types, the columns and a general statistical description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in simple csv file\n",
    "data_1 = pd.read_csv('./data/input/mobile_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount of rows and columns \n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much memory does the dataframe consume\n",
    "data_1.info()\n",
    "# if column contains \"object\" data types, use \n",
    "# data_1.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - notice when you overwrite the existing dataframe, or when are you creating a new df\n",
    "data_1 = data_1.rename(columns = {'blue' : 'bluetooth', \n",
    "                              'fc' : 'fc_megapixel',\n",
    "                              'pc' : 'pc_megapixel',\n",
    "                              'm_dep' : 'm_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify display behaviour\n",
    "pd.get_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = data_1.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1.drop_duplicates()\n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing Nans with 0\n",
    "_data imputation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['fc_megapixel'] = data_1['fc_megapixel'].fillna(0)\n",
    "\n",
    "data_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.sort_values(by=['fc_megapixel']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Forward or Backward\n",
    "If we supply a method parameter to the fillna() method, we can fill forward or backward as we need. To fill forward, use the methods pad or fill, and to fill backward, use bfill and backfill.\n",
    "\n",
    "NB! Make sure this makes sense for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[data_1['ram'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_1['ram'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['ram'] = data_1['ram'].fillna(method='backfill')\n",
    "\n",
    "len(data_1['ram'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing nan with median of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['mobile_wt'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['mobile_wt'] = data_1['mobile_wt'].fillna(data_1['mobile_wt'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['mobile_wt'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers\n",
    "Another type of profiling is to check for legitimate values and handling outliers. We need to do that with numerical values only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data_1.drop(['id','phone_id', 'bluetooth', 'dual_sim', 'four_g', 'three_g', \n",
    "                          'touch_screen', 'wifi', 'price_range'], axis=1)\n",
    "numerical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = data_1[['id', 'phone_id','bluetooth', 'dual_sim', 'four_g', 'three_g', \n",
    "                         'touch_screen', 'wifi', 'price_range']]\n",
    "categorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a single column's outliers\n",
    "sns.boxplot(data=numerical_data['ram'],\n",
    "            orient = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all columns\n",
    "bp = sns.boxplot(data = numerical_data)\n",
    "\n",
    "bp.set_xticklabels(bp.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize, you may want to standardize the values of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_array = scaler.fit_transform(numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = pd.DataFrame(scaled_array, columns = numerical_data.columns)\n",
    "\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = sns.boxplot(data = scaled_data)\n",
    "\n",
    "bp.set_xticklabels(bp.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use Interquartile range (IQR) to distinguish outliers \n",
    "Q1 = numerical_data.quantile(0.25)\n",
    "Q3 = numerical_data.quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_removed_data = numerical_data[~ ((numerical_data < (Q1 - 1.5 * IQR)) \\\n",
    "                                     | (numerical_data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "outliers_removed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = sns.boxplot(data = outliers_removed_data)\n",
    "\n",
    "bp.set_xticklabels(bp.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_1 = outliers_removed_data.join(categorical_data, how='inner')\n",
    "final_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this part of the data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_1.to_csv('./data/output/mobile_data_1_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the second data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('./data/input/mobile_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some of the columns have compound information, e.g. Body Dimension and Body weight. Also, some records have meaningless data. Also, some columns are irrelevant to the analysis, like the Picture url.\n",
    "So, we will do the following.\n",
    "- Drop irrelevant columns, e.g. Picture url\n",
    "- Drop records with meaningless data. We can drop records for which columns like Body dimensions, Body weight and display resolution are less than 20 characters long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_2.drop('Picture URL small', axis =1)\n",
    "data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_2.rename (columns = {'Body Dimensions':'BodyDimensions', 'Body Weight':'BodyWeight', 'Display Resolution':'DisplayResolution'})\n",
    "data_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.info()\n",
    "#data_2.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that Pandas is not able to infer the type on its own for the non-ID columns. So, let's enforce the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['Brand']= data_2['Brand'].astype('str')\n",
    "data_2['Phone']= data_2['Phone'].astype('str')\n",
    "data_2['BodyDimensions']= data_2['BodyDimensions'].astype('str')\n",
    "data_2['BodyWeight']= data_2['BodyWeight'].astype('str')\n",
    "data_2['DisplayResolution']= data_2['DisplayResolution'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_2 = data_2[(data_2['BodyWeight'].str.len() > 10) & (data_2['DisplayResolution'].str.len() > 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_2.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the weight as a new column for the value of grams only. We can achieve that by assuming that we split the string describing the weight by the **g** character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeightInGrams = meaningful_data_2.apply(lambda row: row['BodyWeight'].split('g')[0].strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeightInGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_2['WeightInGrams'] = WeightInGrams \n",
    "# make a copy beforehand to avoid circular reference and/or making unwanted changes in previous dataframes\n",
    "# meaningful_data_3 = meaningful_data_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_3.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice in the original data that the ''BodyWeight'' column has actually multiple values. For example, we can find value like _331 g (Wi-Fi) / 341 g (3G/LTE) (11.68 oz);_. This means that there are different weights for different configurations. Our objective is to create a new row for each different weight and put another column called configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is to first get those rows that have multiple weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones = meaningful_data_3[(meaningful_data_3['BodyWeight'].str.find('/') > -1 ) | (meaningful_data_3['BodyWeight'].str.find(',') > -1 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first unify the splitting character. We make sure that ',' is transformed to '/'. Also, we have another occurrence of '/' in some configurations e.g., (3G/LTE). In this case, this will complicate the splitting. The relevant occurrence of weight separators is followed by a space. So, we make another replacement where we replace '/ ' with '//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unifiedSeparator = multiple_weight_phones.apply (lambda row: row['BodyWeight'].replace(',','/').replace('/ ','//'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unifiedSeparator.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones = multiple_weight_phones.drop(['BodyWeight'], axis=1)\n",
    "multiple_weight_phones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones['BodyWeight'] =unifiedSeparator\n",
    "multiple_weight_phones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_phones['BodyWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weight_2 = multiple_weight_phones.apply(lambda row: row['BodyWeight'].split('//'), axis=1).explode()\n",
    "multiple_weight_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3 = multiple_weight_phones.join(pd.DataFrame(multiple_weight_2,columns=['Config']))\n",
    "multiple_weights_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3 = multiple_weights_3.drop(['Brand', 'Phone', 'BodyDimensions', 'DisplayResolution',\n",
    "       'WeightInGrams', 'BodyWeight'], axis=1)\n",
    "multiple_weights_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the config column to weight and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = multiple_weights_3.apply(lambda row: row['Config'].split('g')[0].strip(), axis=1)\n",
    "weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = multiple_weights_3.apply(lambda row: row['Config'].split('g')[1].strip().split(')')[0][1:], axis=1)\n",
    "config.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3= multiple_weights_3.drop(['Config'], axis = 1)\n",
    "multiple_weights_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3['Config'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3['Weight'] = weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4 = meaningful_data_3.join(multiple_weights_3, how='left',rsuffix='_multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4 = meaningful_data_4.drop(['ID_multiple'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4['Weight'].fillna(meaningful_data_4['WeightInGrams'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4 = meaningful_data_4.drop(['WeightInGrams'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_data_4.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dataframes held in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the memory footprint\n",
    "df_list = []    \n",
    "for var in dir():\n",
    "    if isinstance(locals()[var], pd.core.frame.DataFrame) and var[0] != '_':\n",
    "        df_list.append(var)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cons = 0\n",
    "for d in df_list:\n",
    "    memory_cons += locals()[d].memory_usage(deep=True).sum()\n",
    "\n",
    "print(f'Total memory consumed by dataframes: {round(memory_cons/1024/1024,1)} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the empty \"Config\" column in the meaningful_data_4 dataframe with value \"Standard\"\n",
    "\n",
    "config_values = meaningful_data_4['Config'].fillna('Standard')\n",
    "meaningful_data_5 = meaningful_data_4\n",
    "meaningful_data_5['Config'] = config_values\n",
    "meaningful_data_5.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the body dimensions and get separate height, width, and depth dimensions in mm\n",
    "\n",
    "dimensions = meaningful_data_5.apply(lambda row: row['BodyDimensions'].split('mm')[0].strip(), axis=1)\n",
    "dimensions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_dim = dimensions.apply(lambda row: row.split(' x ')[0].strip())\n",
    "width_dim = dimensions.apply(lambda row: row.split(' x ')[1].strip())\n",
    "depth_dim = dimensions.apply(lambda row: row.split(' x ')[2].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_dim.head()\n",
    "#width_dim.head()\n",
    "#depth_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_2 = (meaningful_data_5\n",
    "                     .join(pd.DataFrame(height_dim, columns=['Height']))\n",
    "                     .join(pd.DataFrame(width_dim, columns=['Width']))\n",
    "                     .join(pd.DataFrame(depth_dim, columns=['Depth']))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join the three data sets:\n",
    "## preprocessed mobile 1 dataset\n",
    "## preprocessed mobile 2 dataset\n",
    "## price ranges\n",
    "\n",
    "\n",
    "price_ranges = pd.read_csv('./data/input/price_ranges.csv', header=0, names=['price_range', 'Min', 'Max'])\n",
    "\n",
    "joined_data = (final_data_1\n",
    "               .join(final_data_2,how='inner')\n",
    "               .join(price_ranges,how='inner',on='price_range',rsuffix='_r')\n",
    "              )\n",
    "\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final data frame in a file 'ready_for_analysis.csv'\n",
    "\n",
    "joined_data.to_csv('./data/output/ready_for_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check also:\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/docs/reference/index.html\">API reference</a>\n",
    "* <a href=\"https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\">Official cheat sheet</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
